{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-70b1b7983851>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-70b1b7983851>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    def Winner():\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class HexEnv(object):\n",
    "    kEmpty = 0\n",
    "    kHorizontal = 1\n",
    "    kVertical = 2\n",
    "    def __init__(self, board_size):\n",
    "        self.board_size = board_size\n",
    "        self.reset()\n",
    "\n",
    "    def winner(self):\n",
    "        return HexEnv.kEmpty\n",
    "\n",
    "    def to_move(self):\n",
    "        return self.to_move\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_move(board, action, player):\n",
    "        # TODO implement\n",
    "        coords = HexEnv.action_to_coordinate(board, action)\n",
    "        \n",
    "        board[2, coords[0], coords[1]] = 0\n",
    "        board[player, coords[0], coords[1]] = 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.zeros((3, self.board_size, self.board_size))\n",
    "        self.state[2, :, :] = 1.0\n",
    "        self.to_play = HexEnv.kHorizontal\n",
    "        self.done = False\n",
    "\n",
    "        # Let the opponent play if it's not the agent's turn\n",
    "        if self.player_color != self.to_play:\n",
    "            a = self.opponent_policy(self.state)\n",
    "            HexEnv.make_move(self.state, a, HexEnv.kHorizontal)\n",
    "            self.to_play = HexEnv.kVertical\n",
    "        return self.state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    def __init__(self):\n",
    "        # TODO set up network\n",
    "        self.inputs = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "        W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "        self.prob_output_layer = tf.matmul(inputs1,W)\n",
    "        #self.max_action_layer = tf.argmax(prob_output_layer,1)\n",
    "       \n",
    "    @staticmethod\n",
    "    def rargmax(vector):\n",
    "        \"\"\" Argmax that chooses randomly among eligible maximum indices. \"\"\"\n",
    "        m = np.amax(vector)\n",
    "        indices = np.nonzero(vector == m)[0]\n",
    "        return random.choice(indices)\n",
    "        \n",
    "    def evaluate_prob_dist(self, sess, state):\n",
    "        prob_output = sess.run([self.prob_output_layer],feed_dict={self.inputs:state})\n",
    "        return prob_output\n",
    "    \n",
    "    def exploration_policy(self, sess, state):\n",
    "        played = np.logical_or(state[HexEnv.kVertical], state[HexEnv.kHorizontal]).flatten()\n",
    "        state = np.asarray(state, dtype=theano.config.floatX)\n",
    "        prob_dist = self.evaluate_prob_dist(sess, state)\n",
    "\n",
    "        #epsilon greedy\n",
    "        if np.random.rand()<0.1:\n",
    "            action = np.random.choice(np.where(played==0)[0])\n",
    "            return action, prob_dist\n",
    "\n",
    "        values = np.copy(prob_dist)\n",
    "        #never select played values\n",
    "        values[played]=-2\n",
    "        action = Learner.rargmax(values)\n",
    "        return action, prob_dist\n",
    "    \n",
    "    \n",
    "    def learn(self, sess, state, batch_size):\n",
    "        #Do nothing if we don't yet have enough entries in memory for a full batch\n",
    "        if(self.mem.size < batch_size):\n",
    "            return\n",
    "        \n",
    "        # TODO implement batch sampling\n",
    "        states1, actions, states2, terminals = self.mem.sample_batch(batch_size)\n",
    "\n",
    "        # \n",
    "        Pw2= self._evaluate_Pws(states2)\n",
    "        #add a cap on the lowest possible value of losing probability\n",
    "        Pl =  np.maximum(1-Pw2,0.00001)\n",
    "        joint = np.min(Pl, axis=1)\n",
    "        #Update networks\n",
    "        Pw_targets = np.zeros(terminals.size).astype(theano.config.floatX)\n",
    "        Pw_targets[terminals==0] = joint[terminals==0]\n",
    "        Pw_targets[terminals==1] = 1\n",
    "        return self._update(states1, actions, Pw_targets)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:  [ 3.  3.  3.  3.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SIZE = 11\n",
    "\n",
    "# We'll bundle groups of examples during training for efficiency.\n",
    "# This defines the size of the batch.\n",
    "BATCH_SIZE = 1\n",
    "# We have only one channel in our grayscale images.\n",
    "NUM_CHANNELS = 3\n",
    "# The random seed that defines initialization.\n",
    "SEED = 42\n",
    "\n",
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step, which we'll write once we define the graph structure.\n",
    "train_data_node = tf.placeholder(\n",
    "  tf.float32,\n",
    "  shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.float32,\n",
    "                                   shape=(BATCH_SIZE, NUM_LABELS))\n",
    "\n",
    "# For the validation and test data, we'll just hold the entire dataset in\n",
    "# one constant node.\n",
    "validation_data_node = tf.constant(validation_data)\n",
    "test_data_node = tf.constant(test_data)\n",
    "\n",
    "# The variables below hold all the trainable weights. For each, the\n",
    "# parameter defines how the variables will be initialized.\n",
    "conv1_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, depth 32.\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv1_biases = tf.Variable(tf.zeros([32]))\n",
    "conv2_weights = tf.Variable(\n",
    "  tf.truncated_normal([5, 5, 32, 64],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth 512.\n",
    "  tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512]))\n",
    "fc2_weights = tf.Variable(\n",
    "  tf.truncated_normal([512, NUM_LABELS],\n",
    "                      stddev=0.1,\n",
    "                      seed=SEED))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-79bcae2670e7>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-79bcae2670e7>\"\u001b[0;36m, line \u001b[0;32m52\u001b[0m\n\u001b[0;31m    Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(16)[next_game_state:next_game_state+1]})\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#These lines establish the feed-forward part of the network used to choose actions\n",
    "inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "Qout = tf.matmul(inputs1,W)\n",
    "predict = tf.argmax(Qout,1)\n",
    "\n",
    "#Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "updateModel = trainer.minimize(loss)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Set learning parameters\n",
    "y = .99\n",
    "e = 0.1\n",
    "num_episodes = 2000\n",
    "BOARD_SIZE = 11\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "game_lengths = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    env = HexEnv(BOARD_SIZE)\n",
    "    agent = Learner()\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        #Reset environment and get first new observation\n",
    "        game_state = env.reset()\n",
    "        game_length = 0\n",
    "        to_move = HexEnv.kHorizontal\n",
    "        \n",
    "        while game_state.Winner() != HexEnv.kEmpty:\n",
    "            game_length += 1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            action, prob_dist = agent.exploration_policy(sess, game_state)\n",
    "                \n",
    "            #Get new state and reward from environment\n",
    "            next_game_state = env.make_move(action, game_state, to_move)\n",
    "            terminal = 0\n",
    "            if game_state.winner() != HexEnv.kEmpty:\n",
    "                terminal = 1\n",
    "                \n",
    "            # TODO flip board\n",
    "            agent.update_memory(game_state, action, next_game_state, terminal)\n",
    "            prob_dist_costs = agent.learn(sess, batch_size)\n",
    "\n",
    "            \n",
    "            #Obtain maxQ' and set our target value for chosen action.\n",
    "            maxQ1 = np.max(Q1)\n",
    "            targetQ = allQ\n",
    "            targetQ[0,a[0]] = reward + y*maxQ1\n",
    "            \n",
    "            #Train our network using target and predicted Q values\n",
    "            _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[game_state:game_state+1],nextQ:targetQ})\n",
    "            game_state = next_game_state\n",
    "                        if(costs is not None):\n",
    "                Pw_cost, Qsigma_cost = costs\n",
    "            else:\n",
    "                Pw_cost = 0\n",
    "                Qsigma_cost = 0\n",
    "\n",
    "            #update running sums for this episode\n",
    "            num_step += 1\n",
    "            Pw_var_sum += np.mean(((1-Pw)*Pw)[np.logical_not(played)])\n",
    "            Qsigma_sum += np.mean(Qsigma[np.logical_not(played)])\n",
    "            Qsigma_cost_sum += Qsigma_cost\n",
    "            Pw_cost_sum += Pw_cost\n",
    "            \n",
    "        game_lengths.append(game_length)\n",
    "\n",
    "print \"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
